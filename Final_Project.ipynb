{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywQPHLj1drbL"
      },
      "source": [
        "# Import Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFJ-CVaTIP_l"
      },
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sklearn\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.optim import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DDhA_JEkk88"
      },
      "source": [
        "myseed = 0  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(myseed)\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "120WsZ4ZKMT0",
        "outputId": "6ab1e6bb-3138-4a89-c242-264482d79d58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOQKjAaybN1j"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y86YkhsxMOdn"
      },
      "source": [
        "這邊已經將data全部都處理好了\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OlcKMqrLVbl"
      },
      "source": [
        "class myDataset(Dataset):\n",
        "  def __init__(self, path , mode='train'):\n",
        "    self.mode=mode\n",
        "    data=pd.read_csv(path)\n",
        "    data=data.fillna('N')\n",
        "    # data=data.drop(['flbmk','flg_3dsmk'],axis=1)#missing values直接整個feature drop掉\n",
        "    data = pd.get_dummies(data)#one-hot encoding將非數字轉成1數字\n",
        "\n",
        "    if mode in ['train','val']:\n",
        "\n",
        "      ##平衡data數量\n",
        "      positive_indices = np.array(data[data.fraud_ind == 1].index)#盜刷idx 較少\n",
        "\n",
        "      normal_indices = data[data.fraud_ind == 0].index#沒盜刷idx\n",
        "      random_normal_indices = np.random.choice(normal_indices, len(positive_indices), replace = False)#從沒盜刷中隨機抽樣，使得 盜刷和沒盜刷一樣多\n",
        "      random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "      under_sample_indices = np.concatenate([positive_indices,random_normal_indices])#合併idx\n",
        "      data=data.iloc[under_sample_indices , :]\n",
        "\n",
        "      ####################################################################\n",
        "      target=data['fraud_ind']\n",
        "      data=data.drop('fraud_ind',axis=1)#將target從train中刪除\n",
        "\n",
        "      data=data.values#將data轉成numpy\n",
        "      target=target.values#將target轉成numpy\n",
        "\n",
        "      #打亂data，怕data有按照順序 ，target要記得也改 ，他們是一組的\n",
        "      random_index=[i  for i in range(len(data))]\n",
        "      random.shuffle(random_index)\n",
        "      data=data[random_index]\n",
        "      target=target[random_index]\n",
        "\n",
        "      if mode=='train':\n",
        "        indices=[i  for i in range(len(data)) if i%10 != 0 ]\n",
        "      elif mode=='val':\n",
        "        indices=[i  for i in range(len(data)) if i%10 == 0 ]\n",
        "      self.data =torch.FloatTensor( data[indices] )\n",
        "      self.target =torch.LongTensor( target[indices] )\n",
        "\n",
        "      print(f'{mode}_data 有{self.data.shape[0]}筆資料，每筆 dim={self.data.shape[1]}')\n",
        "    elif mode == 'test':\n",
        "      target=data['fraud_ind']\n",
        "      data=data.drop('fraud_ind',axis=1)#將target從test中刪除\n",
        "\n",
        "      data=data.values#將data轉成numpy\n",
        "      target=target.values#將target轉成numpy\n",
        "\n",
        "      self.target =torch.LongTensor(target)\n",
        "      self.data=torch.FloatTensor(data)\n",
        "\n",
        "      print(f'{mode}_data 有{self.data.shape[0]}筆資料，每筆 dim={self.data.shape[1]}')\n",
        "    self.dim = data.shape[1]\n",
        "    #Normorlization\n",
        "    self.data[:, :] = \\\n",
        "            (self.data[:, :] - self.data[:, :].mean(dim=0, keepdim=True)) \\\n",
        "            / self.data[:, :].std(dim=0, keepdim=True)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.mode in ['train','val'] :\n",
        "      return self.data[idx],self.target[idx]\n",
        "    else:\n",
        "      return self.data[idx],self.target[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4qmA60XIN7h"
      },
      "source": [
        "def getDataLoader(path , mode ,batch_size):\n",
        "  dataset = myDataset(path,mode)\n",
        "  dataloader = DataLoader(dataset ,batch_size ,shuffle=(mode=='train') ,drop_last=True ,pin_memory=True)\n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV5RQHSab8xT",
        "outputId": "16a5b2f5-8eff-41fa-d71c-ff2d4d59686f"
      },
      "source": [
        "tr_path=\"/content/gdrive/MyDrive/Colab Notebooks/金融科技導論/team/train.csv\"\n",
        "train_set = getDataLoader(tr_path,mode='train',batch_size=250)\n",
        "val_set = getDataLoader(tr_path,mode='val',batch_size=250)\n",
        "\n",
        "test_path=\"/content/gdrive/MyDrive/Colab Notebooks/金融科技導論/team/test.csv\"\n",
        "test_set = getDataLoader(test_path,mode='test',batch_size=250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data 有27509筆資料，每筆 dim=27\n",
            "val_data 有3057筆資料，每筆 dim=27\n",
            "test_data 有380447筆資料，每筆 dim=27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6zUMqp3qJPP"
      },
      "source": [
        "# Cost Sensitive loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8zWdyHDqZjY"
      },
      "source": [
        "ref:https://github.com/agaldran/cost_sensitive_loss_classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ-W97X3qTDF"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.stats as stats2\n",
        "import sys\n",
        "try:\n",
        "    from kornia.losses import FocalLoss as focal_loss\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def get_gauss_label(label, n_classes, amplifier, noise=0):\n",
        "    n = n_classes*amplifier\n",
        "    half_int = amplifier/2\n",
        "    label_noise = np.random.uniform(low=-noise, high=noise)\n",
        "    if label == 0:\n",
        "        label_noise = np.abs(label_noise)\n",
        "    if label == 4:\n",
        "        label_noise = -np.abs(label_noise)\n",
        "    label += label_noise\n",
        "    label_new = half_int + label*amplifier\n",
        "    gauss_label = stats2.norm.pdf(np.arange(n), label_new, half_int/2)\n",
        "    gauss_label/=np.sum(gauss_label)\n",
        "    return gauss_label\n",
        "\n",
        "def get_gaussian_label_distribution(n_classes, std=0.5):\n",
        "    cls = []\n",
        "    for n in range(n_classes):\n",
        "        cls.append(stats2.norm.pdf(range(n_classes), 0, std))\n",
        "    dists = np.stack(cls, axis=0)\n",
        "    return dists\n",
        "\n",
        "def cross_entropy_loss_one_hot(logits, target, reduction='mean'):\n",
        "    logp = F.log_softmax(logits, dim=1)\n",
        "    loss = torch.sum(-logp * target, dim=1)\n",
        "    if reduction == 'none':\n",
        "        return loss\n",
        "    elif reduction == 'mean':\n",
        "        return loss.mean()\n",
        "    elif reduction == 'sum':\n",
        "        return loss.sum()\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            '`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n",
        "\n",
        "def one_hot_encoding(label, n_classes):\n",
        "    return torch.zeros(label.size(0), n_classes).to(label.device).scatter_(\n",
        "        1, label.view(-1, 1), 1)\n",
        "\n",
        "def label_smoothing_criterion(alpha=0.1, distribution='uniform', std=0.5, reduction='mean'):\n",
        "    def _label_smoothing_criterion(logits, labels):\n",
        "        n_classes = logits.size(1)\n",
        "        device = logits.device\n",
        "        # manipulate labels\n",
        "        one_hot = one_hot_encoding(labels, n_classes).float().to(device)\n",
        "        if distribution == 'uniform':\n",
        "            uniform = torch.ones_like(one_hot).to(device)/n_classes\n",
        "            soft_labels = (1 - alpha)*one_hot + alpha*uniform\n",
        "        elif distribution == 'gaussian':\n",
        "            dist = get_gaussian_label_distribution(n_classes, std=std)\n",
        "            soft_labels = torch.from_numpy(dist[labels.cpu().numpy()]).to(device)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        loss = cross_entropy_loss_one_hot(logits, soft_labels.float(), reduction)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    return _label_smoothing_criterion\n",
        "\n",
        "def cost_sensitive_loss(input, target, M):\n",
        "    if input.size(0) != target.size(0):\n",
        "        raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
        "                         .format(input.size(0), target.size(0)))\n",
        "    device = input.device\n",
        "    M = M.to(device)\n",
        "    return (M[target, :]*input.float()).sum(axis=-1)\n",
        "    # return torch.diag(torch.matmul(input, M[:, target]))\n",
        "\n",
        "class CostSensitiveLoss(nn.Module):\n",
        "    def __init__(self,  n_classes, exp=1, normalization='softmax', reduction='mean'):\n",
        "        super(CostSensitiveLoss, self).__init__()\n",
        "        if normalization == 'softmax':\n",
        "            self.normalization = nn.Softmax(dim=1)\n",
        "        elif normalization == 'sigmoid':\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        else:\n",
        "            self.normalization = None\n",
        "        self.reduction = reduction\n",
        "        x = np.abs(np.arange(n_classes, dtype=np.float32))\n",
        "        M = np.abs((x[:, np.newaxis] - x[np.newaxis, :])) ** exp\n",
        "        M /= M.max()\n",
        "        self.M = torch.from_numpy(M)\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        preds = self.normalization(logits)\n",
        "        loss = cost_sensitive_loss(preds, target, self.M)\n",
        "        if self.reduction == 'none':\n",
        "            return loss\n",
        "        elif self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            raise ValueError('`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n",
        "\n",
        "class CostSensitiveRegularizedLoss(nn.Module):\n",
        "    def __init__(self,  n_classes=5, exp=2, normalization='softmax', reduction='mean', base_loss='ce', lambd=10):\n",
        "        super(CostSensitiveRegularizedLoss, self).__init__()\n",
        "        if normalization == 'softmax':\n",
        "            self.normalization = nn.Softmax(dim=1)\n",
        "        elif normalization == 'sigmoid':\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        else:\n",
        "            self.normalization = None\n",
        "        self.reduction = reduction\n",
        "        x = np.abs(np.arange(n_classes, dtype=np.float32))\n",
        "        M = np.abs((x[:, np.newaxis] - x[np.newaxis, :])) ** exp\n",
        "\n",
        "        M /= M.max()\n",
        "        self.M = torch.from_numpy(M)\n",
        "        self.lambd = lambd\n",
        "        self.base_loss = base_loss\n",
        "\n",
        "        if self.base_loss == 'ce':\n",
        "            self.base_loss = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
        "        elif self.base_loss == 'ls':\n",
        "            self.base_loss = label_smoothing_criterion(distribution='uniform', reduction=reduction)\n",
        "        elif self.base_loss == 'gls':\n",
        "            self.base_loss = label_smoothing_criterion(distribution='gaussian', reduction=reduction)\n",
        "        elif self.base_loss == 'focal_loss':\n",
        "            kwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": reduction}\n",
        "            self.base_loss = focal_loss(**kwargs)\n",
        "        else:\n",
        "            sys.exit('not a supported base_loss')\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        base_l = self.base_loss(logits, target)\n",
        "        if self.lambd == 0:\n",
        "            return self.base_loss(logits, target)\n",
        "        else:\n",
        "            preds = self.normalization(logits)\n",
        "            loss = cost_sensitive_loss(preds, target, self.M)\n",
        "            if self.reduction == 'none':\n",
        "                return base_l + self.lambd*loss\n",
        "            elif self.reduction == 'mean':\n",
        "                return base_l + self.lambd*loss.mean()\n",
        "            elif self.reduction == 'sum':\n",
        "                return base_l + self.lambd*loss.sum()\n",
        "            else:\n",
        "                raise ValueError('`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n",
        "\n",
        "def get_cost_sensitive_criterion(n_classes=5, exp=2):\n",
        "    train_criterion = CostSensitiveLoss(n_classes, exp=exp, normalization='softmax')\n",
        "    val_criterion = CostSensitiveLoss(n_classes, exp=exp, normalization='softmax')\n",
        "    return train_criterion, val_criterion\n",
        "\n",
        "def get_cost_sensitive_regularized_criterion(base_loss='ce', n_classes=5, lambd=1, exp=2):\n",
        "    train_criterion = CostSensitiveRegularizedLoss(n_classes, exp=exp, normalization='softmax', base_loss=base_loss, lambd=lambd)\n",
        "    val_criterion = CostSensitiveRegularizedLoss(n_classes, exp=exp, normalization='softmax', base_loss=base_loss, lambd=lambd)\n",
        "\n",
        "    return train_criterion, val_criterion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUhVSUUyMVIu"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zduTKeqmMwGO"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self,input_dim=27):\n",
        "    super(NN, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 2),\n",
        "          )\n",
        "  def forward(self, x):\n",
        "    x=self.net(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpoYG2uMTQUd"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-7zIivTfdm"
      },
      "source": [
        "model=NN(train_set.dataset.dim).to(device)\n",
        "\n",
        "num_epochs = 20\n",
        "learning_rate = 1e-4\n",
        "##loss1\n",
        "# criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "#loss2\n",
        "# criterion = CostSensitiveLoss(n_classes)\n",
        "# criterion.M = torch.tensor([[0.,1.],\\\n",
        "#               [1000.,0.]]) #cost-matrix\n",
        "#loss3\n",
        "criterion = CostSensitiveRegularizedLoss(n_classes=2, base_loss='ce', lambd=10)\n",
        "criterion.M = torch.tensor([[0.,1.],\\\n",
        "              [1000.,0.]]) #cost-matrix\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters() , lr=learning_rate ,weight_decay=1e-3 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YLF-pBW4Wj32",
        "outputId": "2219142e-13d5-4f83-b532-85e39f3a5b6f"
      },
      "source": [
        "steps=[]\n",
        "train_F1_score=[]\n",
        "val_F1_score=[]\n",
        "step=0\n",
        "for epoch in range(num_epochs):\n",
        "  steps.append(step)\n",
        "  step+=1\n",
        "  ######## trainning #########\n",
        "  model.train()\n",
        "  train_loss = []\n",
        "  train_accs = []\n",
        "\n",
        "  num_FP=0#統計每個epoch的FP數\n",
        "  num_FN=0\n",
        "  num_TP=0\n",
        "  num_TN=0\n",
        "  for i , data in enumerate(train_set):\n",
        "    inputs , labels = data\n",
        "    inputs , labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    logits = model(inputs)#預測\n",
        "    #計算loss\n",
        "    loss = criterion( logits , labels )#計算和真實label的loss\n",
        "\n",
        "\n",
        "    prediction=logits.argmax(dim=1)#;print(prediction);print(labels)\n",
        "    # TP = (  (prediction==1).int() == (labels==1).int() ).sum() #.item() #計算每個batch的TP數\n",
        "    # FP = (  (prediction==1).int() == (labels==0).int() ).sum().float() #.item()\n",
        "    # FN = (  (prediction==0).int() == (labels==1).int() ).sum().float() #.item()\n",
        "    # num_FP += FP.detach().clone()\n",
        "    # num_FN += FN.detach().clone()\n",
        "    # num_TP += TP.detach().clone()\n",
        "\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "      if prediction[i]==1 and labels[i]==1:\n",
        "        num_TP+=1\n",
        "      elif prediction[i]==1 and labels[i]==0:\n",
        "        num_FP+=1\n",
        "      elif prediction[i]==0 and labels[i]==1:\n",
        "        num_FN+=1\n",
        "      else:\n",
        "        num_TN+=1\n",
        "\n",
        "\n",
        "    # - , predicetion = torch.max( logits , 1)\n",
        "    # acc = (predicetion==labels).sum().item()\n",
        "\n",
        "    optimizer.zero_grad()#gradient歸0\n",
        "    loss.backward()#計算gradient\n",
        "    optimizer.step()#更新optimizer\n",
        "\n",
        "    acc = ( prediction == labels ).float().mean()\n",
        "    train_loss.append(loss)\n",
        "    train_accs.append(acc)\n",
        "  print(f'num_TP : {num_TP},num_FP : {num_FP},num_FN : {num_FN},num_TN : {num_TN} ')\n",
        "  try:\n",
        "    Precision=num_TP/(num_TP+num_FP)\n",
        "    Recall=num_TP/(num_TP+num_FN)\n",
        "    F1_socre=2*(Precision*Recall)/(Precision+Recall)\n",
        "  except:\n",
        "    Precision=0\n",
        "    Recall=0\n",
        "    F1_socre=0\n",
        "    print(\"train: TP,FP,FN中某數為0\")\n",
        "\n",
        "  train_loss=sum(train_loss)/len(train_loss)#計算平均\n",
        "  train_acc=sum(train_accs)/len(train_accs)#計算平均\n",
        "\n",
        "  train_F1_score.append(F1_socre)#畫圖用\n",
        "  print(f'[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_loss.item():.5f}, acc = {train_acc.item():.5f}, F1_socre = {F1_socre:.5f}, Precision = {Precision:.5f} , Recall = {Recall:.5f}')\n",
        "\n",
        "\n",
        "\n",
        "  ######## Validation #########\n",
        "  valid_loss = []\n",
        "  valid_accs = []\n",
        "  # TP=0  #真實為true且預測為true\n",
        "  # FP=0 #真實為false且預測為true\n",
        "  # FN=0  #真實為true且預測為false\n",
        "  num_FP=0\n",
        "  num_FN=0\n",
        "  num_TP=0\n",
        "  model.eval()\n",
        "  for i , data in enumerate(val_set):\n",
        "    inputs , labels = data\n",
        "    inputs , labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs)\n",
        "    loss = criterion(logits,labels)\n",
        "\n",
        "    prediction=logits.argmax(dim=1)\n",
        "    for i in range(len(labels)):\n",
        "      if prediction[i]==1 and labels[i]==1:\n",
        "        num_TP+=1\n",
        "      elif prediction[i]==1 and labels[i]==0:\n",
        "        num_FP+=1\n",
        "      elif prediction[i]==0 and labels[i]==1:\n",
        "        num_FN+=1\n",
        "\n",
        "    acc = ( prediction == labels ).float().mean()\n",
        "    valid_loss.append(loss)\n",
        "    valid_accs.append(acc)\n",
        "  try:\n",
        "    Precision=num_TP/(num_TP+num_FP)\n",
        "    Recall=num_TP/(num_TP+num_FN)\n",
        "    F1_socre=2*(Precision*Recall)/(Precision+Recall)\n",
        "\n",
        "  except:\n",
        "    Precision=0\n",
        "    Recall=0\n",
        "    F1_socre=0\n",
        "    print(\"test : TP,FP,FN中某數為0\")\n",
        "\n",
        "  valid_loss=sum(valid_loss)/len(valid_loss)\n",
        "  valid_acc=sum(valid_accs)/len(valid_accs)\n",
        "  val_F1_score.append(F1_socre)\n",
        "  print(f\"[ Valid | {epoch + 1:03d}/{num_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}, F1_socre = {F1_socre:.5f}, Precision = {Precision:.5f} , Recall = {Recall:.5f}'\")\n",
        "  print(' ')\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(steps,train_F1_score,color='black',label=\"train_F1_score\")\n",
        "plt.plot(steps,val_F1_score,color='red',label=\"val_F1_score\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('F1 score')\n",
        "plt.legend()#顯示圖示\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_TP : 6352,num_FP : 4547,num_FN : 7408,num_TN : 9193 \n",
            "[ Train | 001/020 ] loss = 2636.62354, acc = 0.56527, F1_socre = 0.51519, Precision = 0.58281 , Recall = 0.46163\n",
            "[ Valid | 001/020 ] loss = 2536.40283, acc = 0.58667, F1_socre = 0.53938, Precision = 0.59557 , Recall = 0.49287'\n",
            " \n",
            "num_TP : 7252,num_FP : 4306,num_FN : 6512,num_TN : 9430 \n",
            "[ Train | 002/020 ] loss = 2513.39014, acc = 0.60662, F1_socre = 0.57278, Precision = 0.62744 , Recall = 0.52688\n",
            "[ Valid | 002/020 ] loss = 2408.78491, acc = 0.62833, F1_socre = 0.59587, Precision = 0.63919 , Recall = 0.55804'\n",
            " \n",
            "num_TP : 8095,num_FP : 4047,num_FN : 5666,num_TN : 9692 \n",
            "[ Train | 003/020 ] loss = 2389.41650, acc = 0.64680, F1_socre = 0.62502, Precision = 0.66669 , Recall = 0.58826\n",
            "[ Valid | 003/020 ] loss = 2282.34009, acc = 0.66833, F1_socre = 0.64528, Precision = 0.67943 , Recall = 0.61439'\n",
            " \n",
            "num_TP : 8815,num_FP : 3774,num_FN : 4948,num_TN : 9963 \n",
            "[ Train | 004/020 ] loss = 2269.61035, acc = 0.68284, F1_socre = 0.66902, Precision = 0.70021 , Recall = 0.64049\n",
            "[ Valid | 004/020 ] loss = 2158.82104, acc = 0.70300, F1_socre = 0.68901, Precision = 0.70905 , Recall = 0.67006'\n",
            " \n",
            "num_TP : 9447,num_FP : 3577,num_FN : 4315,num_TN : 10161 \n",
            "[ Train | 005/020 ] loss = 2152.64819, acc = 0.71302, F1_socre = 0.70537, Precision = 0.72535 , Recall = 0.68646\n",
            "[ Valid | 005/020 ] loss = 2040.18884, acc = 0.73233, F1_socre = 0.72396, Precision = 0.73329 , Recall = 0.71487'\n",
            " \n",
            "num_TP : 9990,num_FP : 3386,num_FN : 3772,num_TN : 10352 \n",
            "[ Train | 006/020 ] loss = 2040.81372, acc = 0.73971, F1_socre = 0.73624, Precision = 0.74686 , Recall = 0.72591\n",
            "[ Valid | 006/020 ] loss = 1926.54150, acc = 0.75667, F1_socre = 0.75237, Precision = 0.75186 , Recall = 0.75289'\n",
            " \n",
            "num_TP : 10478,num_FP : 3266,num_FN : 3282,num_TN : 10474 \n",
            "[ Train | 007/020 ] loss = 1933.90698, acc = 0.76189, F1_socre = 0.76193, Precision = 0.76237 , Recall = 0.76148\n",
            "[ Valid | 007/020 ] loss = 1819.86169, acc = 0.78000, F1_socre = 0.77852, Precision = 0.76974 , Recall = 0.78751'\n",
            " \n",
            "num_TP : 10891,num_FP : 3209,num_FN : 2870,num_TN : 10530 \n",
            "[ Train | 008/020 ] loss = 1834.04004, acc = 0.77895, F1_socre = 0.78181, Precision = 0.77241 , Recall = 0.79144\n",
            "[ Valid | 008/020 ] loss = 1720.23840, acc = 0.79700, F1_socre = 0.79680, Precision = 0.78346 , Recall = 0.81059'\n",
            " \n",
            "num_TP : 11199,num_FP : 3161,num_FN : 2565,num_TN : 10575 \n",
            "[ Train | 009/020 ] loss = 1741.54504, acc = 0.79178, F1_socre = 0.79640, Precision = 0.77987 , Recall = 0.81364\n",
            "[ Valid | 009/020 ] loss = 1627.83411, acc = 0.81267, F1_socre = 0.81415, Precision = 0.79368 , Recall = 0.83571'\n",
            " \n",
            "num_TP : 11414,num_FP : 3138,num_FN : 2347,num_TN : 10601 \n",
            "[ Train | 010/020 ] loss = 1655.30566, acc = 0.80055, F1_socre = 0.80627, Precision = 0.78436 , Recall = 0.82945\n",
            "[ Valid | 010/020 ] loss = 1543.25964, acc = 0.82000, F1_socre = 0.82272, Precision = 0.79657 , Recall = 0.85064'\n",
            " \n",
            "num_TP : 11608,num_FP : 3131,num_FN : 2158,num_TN : 10603 \n",
            "[ Train | 011/020 ] loss = 1575.95630, acc = 0.80767, F1_socre = 0.81445, Precision = 0.78757 , Recall = 0.84324\n",
            "[ Valid | 011/020 ] loss = 1465.07751, acc = 0.82900, F1_socre = 0.83252, Precision = 0.80189 , Recall = 0.86558'\n",
            " \n",
            "num_TP : 11768,num_FP : 3140,num_FN : 1996,num_TN : 10596 \n",
            "[ Train | 012/020 ] loss = 1502.20605, acc = 0.81324, F1_socre = 0.82087, Precision = 0.78937 , Recall = 0.85498\n",
            "[ Valid | 012/020 ] loss = 1393.52637, acc = 0.83267, F1_socre = 0.83722, Precision = 0.80137 , Recall = 0.87644'\n",
            " \n",
            "num_TP : 11871,num_FP : 3132,num_FN : 1894,num_TN : 10603 \n",
            "[ Train | 013/020 ] loss = 1434.84460, acc = 0.81724, F1_socre = 0.82529, Precision = 0.79124 , Recall = 0.86240\n",
            "[ Valid | 013/020 ] loss = 1327.70618, acc = 0.83367, F1_socre = 0.83867, Precision = 0.80062 , Recall = 0.88052'\n",
            " \n",
            "num_TP : 11924,num_FP : 3101,num_FN : 1837,num_TN : 10638 \n",
            "[ Train | 014/020 ] loss = 1372.24756, acc = 0.82044, F1_socre = 0.82846, Precision = 0.79361 , Recall = 0.86651\n",
            "[ Valid | 014/020 ] loss = 1267.16150, acc = 0.83800, F1_socre = 0.84292, Precision = 0.80444 , Recall = 0.88527'\n",
            " \n",
            "num_TP : 11968,num_FP : 3042,num_FN : 1795,num_TN : 10695 \n",
            "[ Train | 015/020 ] loss = 1314.73315, acc = 0.82411, F1_socre = 0.83189, Precision = 0.79734 , Recall = 0.86958\n",
            "[ Valid | 015/020 ] loss = 1211.50415, acc = 0.84200, F1_socre = 0.84680, Precision = 0.80814 , Recall = 0.88934'\n",
            " \n",
            "num_TP : 12022,num_FP : 2989,num_FN : 1743,num_TN : 10746 \n",
            "[ Train | 016/020 ] loss = 1261.79187, acc = 0.82793, F1_socre = 0.83556, Precision = 0.80088 , Recall = 0.87337\n",
            "[ Valid | 016/020 ] loss = 1159.98584, acc = 0.84767, F1_socre = 0.85206, Precision = 0.81436 , Recall = 0.89341'\n",
            " \n",
            "num_TP : 12044,num_FP : 2929,num_FN : 1717,num_TN : 10810 \n",
            "[ Train | 017/020 ] loss = 1212.03284, acc = 0.83105, F1_socre = 0.83831, Precision = 0.80438 , Recall = 0.87523\n",
            "[ Valid | 017/020 ] loss = 1112.33508, acc = 0.84900, F1_socre = 0.85326, Precision = 0.81599 , Recall = 0.89409'\n",
            " \n",
            "num_TP : 12080,num_FP : 2886,num_FN : 1681,num_TN : 10853 \n",
            "[ Train | 018/020 ] loss = 1166.70728, acc = 0.83393, F1_socre = 0.84102, Precision = 0.80716 , Recall = 0.87784\n",
            "[ Valid | 018/020 ] loss = 1068.11340, acc = 0.85000, F1_socre = 0.85418, Precision = 0.81711 , Recall = 0.89477'\n",
            " \n",
            "num_TP : 12115,num_FP : 2881,num_FN : 1649,num_TN : 10855 \n",
            "[ Train | 019/020 ] loss = 1124.13208, acc = 0.83527, F1_socre = 0.84249, Precision = 0.80788 , Recall = 0.88019\n",
            "[ Valid | 019/020 ] loss = 1027.21350, acc = 0.85033, F1_socre = 0.85446, Precision = 0.81762 , Recall = 0.89477'\n",
            " \n",
            "num_TP : 12127,num_FP : 2882,num_FN : 1635,num_TN : 10856 \n",
            "[ Train | 020/020 ] loss = 1084.44202, acc = 0.83575, F1_socre = 0.84300, Precision = 0.80798 , Recall = 0.88119\n",
            "[ Valid | 020/020 ] loss = 989.06293, acc = 0.85200, F1_socre = 0.85603, Precision = 0.81937 , Recall = 0.89613'\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys-e0c5spguo"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlu5HMOapjfn"
      },
      "source": [
        "# model.eval()\n",
        "# test_losss = []\n",
        "# test_accs = []\n",
        "# TP=0  #真實為true且預測為true\n",
        "# FP=0 #真實為false且預測為true\n",
        "# FN=0  #真實為true且預測為false\n",
        "# for i , data in enumerate(test_set):\n",
        "#   inputs , labels = data\n",
        "#   inputs , labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#   with torch.no_grad():\n",
        "#     logits = model(inputs)\n",
        "#     loss = criterion(logits,labels.long())\n",
        "\n",
        "#     prediction=logits.argmax(dim=1)\n",
        "#     # TP += (  (predicetion==1).int() == (labels==1).int() ).sum().item()\n",
        "#     # FP += (  (predicetion==1).int() == (labels==0).int() ).sum().item()\n",
        "#     # FN += (  (predicetion==0).int() == (labels==1).int() ).sum().item()\n",
        "#     for i in range(len(labels)):\n",
        "#       if prediction[i]==1 and labels[i]==1:\n",
        "#         TP+=1\n",
        "#       elif prediction[i]==1 and labels[i]==0:\n",
        "#         FP+=1\n",
        "#       elif prediction[i]==0 and labels[i]==1:\n",
        "#         FN+=1\n",
        "#     acc = ( prediction == labels ).float().mean()\n",
        "#     test_losss.append(loss)\n",
        "#     test_accs.append(acc)\n",
        "# try:\n",
        "#   Precision=TP/(TP+FP)\n",
        "#   Recall=TP/(TP+FN)\n",
        "#   F1_socre=2*(Precision*Recall)/(Precision+Recall)\n",
        "# except:\n",
        "#   Precision=0\n",
        "#   Recall=0\n",
        "#   F1_socre=0\n",
        "#   print(\"test : TP,FP,FN中某數為0\")\n",
        "\n",
        "# test_loss=sum(test_losss)/len(test_losss)\n",
        "# test_acc=sum(test_accs)/len(test_accs)\n",
        "# print(f\"[ Test : loss = {test_loss:.5f}, acc = {test_acc:.5f},F1_socre = {F1_socre:.5f}, Precision = {Precision:.5f} , Recall = {Recall:.5f}'\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}